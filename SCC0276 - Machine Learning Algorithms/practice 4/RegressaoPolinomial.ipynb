{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RegressaoPolinomial.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"pe-lLjBpQ8xy","colab_type":"text"},"cell_type":"markdown","source":["# Regressão Polinomial Multivariada\n","\n","Supor a função multivariada de segundo grau:\n","\n","$f(x, y) = w_01 + w_1x + w_2y + w_3x^2 + w_4y^2 + w_5xy$\n","\n","Vimos em aula a regressão do tipo:\n","\n","$h(a) = w_0g_0(a) + w_1g_1(a) + w_2g_2(a) + \\dots + w_{n-1}g_{n-1}(a) + w_{n}g_{n}(a)$\n","\n","\n","Considere que $a$ é um vetor e que suas componentes representas as variáveis de entrada, por exemplo $a = (x, y)$.\n","Se definirmos as funções $g$ da forma:\n","- $g_0(x, y) = 1$\n","- $g_1(x, y) = x$\n","- $g_2(x, y) = y$\n","- $g_3(x, y) = x^2$\n","- $g_4(x, y) = y^2$\n","- $g_5(x, y) = xy$\n","\n","Então ao realizar regressão em $h(a)$, estaremos encontrando determinando os coeficientes da $f(x, y)$. \n","\n","---\n","\n","A classe **sklearn.preprocessing.PolynomialFeatures** é utilizada para definir as funções $g$ de forma automática. \n","\n","Suponha como exemplo a $f(x,y)$ acima, com $x=x_0$ e $y=x_1$ por conveniência e as observações $O=\\{(1, 0), (3, 26), (7,35), (9, 1), (11, 82), (1, 31)\\}$ de $x_0,x_1$, :\n"]},{"metadata":{"id":"iI0gl-e6OWML","colab_type":"code","outputId":"3c64f99c-67e5-4f2b-9dd4-1e09cf7ac21d","executionInfo":{"status":"ok","timestamp":1552425262374,"user_tz":180,"elapsed":576,"user":{"displayName":"Felipe Padula Sanches","photoUrl":"https://lh4.googleusercontent.com/-nernBhYcW4o/AAAAAAAAAAI/AAAAAAAANRw/LcF4_Q1FmN8/s64/photo.jpg","userId":"18187017750627346096"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"cell_type":"code","source":["from sklearn.preprocessing import PolynomialFeatures\n","import numpy as np\n","\n","# 6 observações de 2 variáveis\n","O = np.array([[1, 0],\n","              [3, 26],\n","              [7, 35],\n","              [9, 1],\n","              [11, 82],\n","              [1, 31]])\n","\n","# Utilizando um polinomio de grau 2\n","p = PolynomialFeatures(degree=2)\n","\n","# Definindo o número de variáveis do polinômio:\n","p.fit(O)\n","\n","# Printando as funções g\n","print(p.get_feature_names())\n","\n","# Aplicando as funções g no nosso conjunto O e imprimindo o seu valor:\n","O_transf = p.transform(O)\n","print(O_transf)\n"],"execution_count":20,"outputs":[{"output_type":"stream","text":["['1', 'x0', 'x1', 'x0^2', 'x0 x1', 'x1^2']\n","[[1.000e+00 1.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00]\n"," [1.000e+00 3.000e+00 2.600e+01 9.000e+00 7.800e+01 6.760e+02]\n"," [1.000e+00 7.000e+00 3.500e+01 4.900e+01 2.450e+02 1.225e+03]\n"," [1.000e+00 9.000e+00 1.000e+00 8.100e+01 9.000e+00 1.000e+00]\n"," [1.000e+00 1.100e+01 8.200e+01 1.210e+02 9.020e+02 6.724e+03]\n"," [1.000e+00 1.000e+00 3.100e+01 1.000e+00 3.100e+01 9.610e+02]]\n"],"name":"stdout"}]},{"metadata":{"id":"TL8CGTEW2ncm","colab_type":"text"},"cell_type":"markdown","source":["Para gerar valores para a regressão, vamos considerar que nossa $f$ possui  $w=( 3, 2, -4, 5, 0, 1)$ e $g=(1, x_0, x_1, x_0^2, x_1^2, x_0x_1) $. Dessa forma temos que  $f(x_0, x_1)= 3 + 2x_0 - 4x_1 + 5x_0^2 + x_0x_1$ . \n","\n","Aplicando-a no nosso conjunto $O$ obtemos os valores:\n","- $f(1, 0) = 10$\n","- $f(3, 26) =  28$\n","- $f(7, 35) = 367$\n","- $f(9, 1) = 431$\n","- $f(11, 82) = 1204$\n","- $f(1, 31) = -83$\n","\n","Tomando $Y=\\{ 10,28,367,431,1204,-83 \\}$, basta fazer a regressão:"]},{"metadata":{"id":"fzNQNEdr7oCy","colab_type":"code","outputId":"b5a44a94-5324-46f0-bcc1-9870cba54a76","executionInfo":{"status":"ok","timestamp":1552426145790,"user_tz":180,"elapsed":627,"user":{"displayName":"Felipe Padula Sanches","photoUrl":"https://lh4.googleusercontent.com/-nernBhYcW4o/AAAAAAAAAAI/AAAAAAAANRw/LcF4_Q1FmN8/s64/photo.jpg","userId":"18187017750627346096"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","\n","# Definindo Y\n","Y = np.array([[10],   \n","              [28],  \n","              [367],  \n","              [431], \n","              [1204],  \n","              [-83]])\n","\n","# Fazemos a regressão nos conjuntos O_transf e Y. Obs: Colocamos \n","# fit_intercept=False pois a nossa g_0 é igual à 1 (ou seja, já \n","# estamos incorporando o termo independente)\n","model = LinearRegression(fit_intercept=False)\n","model.fit(O_transf, Y)\n","\n","# Printando novamente as funções g\n","print(p.get_feature_names())\n","\n","# Printando os coeficientes (w) das funções g. Obs: Arredonda-se\n","# nas primeiras 10 casas para facilitar a leitura\n","print(model.coef_.round(10))\n"],"execution_count":31,"outputs":[{"output_type":"stream","text":["['1', 'x0', 'x1', 'x0^2', 'x0 x1', 'x1^2']\n","[[ 3.  2. -4.  5.  1.  0.]]\n"],"name":"stdout"}]},{"metadata":{"id":"1YLewcjvz0pW","colab_type":"text"},"cell_type":"markdown","source":["Como podemos ver, a regressão estimou corretamente os coeficientes $w$."]}]}